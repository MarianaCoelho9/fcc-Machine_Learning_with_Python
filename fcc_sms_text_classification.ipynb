{"cells":[{"cell_type":"code","source":["!pip uninstall tensorflow\n","!pip uninstall keras"],"metadata":{"id":"knGPy3hy4iHg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install tensorflow==2.12.0\n","!pip install keras"],"metadata":{"id":"wJv-hoxt43Bk"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8RZOuS9LWQvv"},"outputs":[],"source":["# import libraries\n","try:\n","  # %tensorflow_version only exists in Colab.\n","  !pip install tf-nightly\n","except Exception:\n","  pass\n","import tensorflow as tf\n","import pandas as pd\n","#from tensorflow import keras\n","!pip install tensorflow-datasets\n","import tensorflow_datasets as tfds\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","print(tf.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lMHwYXHXCar3"},"outputs":[],"source":["# get data files\n","!wget https://cdn.freecodecamp.org/project-data/sms/train-data.tsv\n","!wget https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv\n","\n","train_file_path = \"train-data.tsv\"\n","test_file_path = \"valid-data.tsv\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g_h508FEClxO"},"outputs":[],"source":["# import csv data into dataframes\n","train = pd.read_csv(train_file_path, sep = '\\t', names = ['label', 'body_text'], header = None)\n","train.dropna()\n","test = pd.read_csv(test_file_path, sep = '\\t', names = ['label', 'body_text'], header = None)\n","test.dropna()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zOMKywn4zReN"},"outputs":[],"source":["train.tail()"]},{"cell_type":"code","source":["test.tail()"],"metadata":{"id":"dFV_N29YMiWW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Remove stopwords\n","import nltk\n","import re\n","from nltk.stem import WordNetLemmatizer\n","\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","stopword = nltk.corpus.stopwords.words('english')\n","lemmatizer = WordNetLemmatizer()\n","\n","def clean_txt(txt):\n","    txt = re.sub(r'([^\\s\\w])+', ' ', txt)\n","    txt = \" \".join([lemmatizer.lemmatize(word) for word in txt.split()\n","                    if not word in stopword])\n","    txt = txt.lower()\n","    return txt\n","\n","train['text_clean'] = train['body_text'].apply(lambda x: clean_txt(x))\n","test['text_clean'] = test['body_text'].apply(lambda x: clean_txt(x))"],"metadata":{"id":"662sm7iDQBXI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train.tail()"],"metadata":{"id":"-BDTjEPSRQeW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test.tail()"],"metadata":{"id":"rjqm7hxY-kNl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_message = train['text_clean']\n","train_label = np.array([0 if x==\"ham\" else 1 for x in train['label']])\n","\n","test_message = test['text_clean']\n","test_label = np.array([0 if x==\"ham\" else 1 for x in test['label']])"],"metadata":{"id":"5K1DkaWN8kyJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(train['body_text'][0])\n","print(train_message[0])"],"metadata":{"id":"GAMS29Ff9akZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocabulary_dict = {}\n","for text in train_message:\n","  for vocabulary in text:\n","    if vocabulary not in vocabulary_dict:\n","      vocabulary_dict[vocabulary] = 1\n","    else:\n","      vocabulary_dict[vocabulary] += 1"],"metadata":{"id":"BB6jexfXXMGM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["VOCAB_SIZE = len(vocabulary_dict)\n","MAX_LENGTH = len(max(train_message))"],"metadata":{"id":"IwWsNH6OXau0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.preprocessing.text import one_hot\n","from keras.preprocessing.sequence import pad_sequences\n","\n","encoded_train_message = [one_hot(d, VOCAB_SIZE) for d in train_message]\n","padded_train_message = pad_sequences(encoded_train_message, maxlen=MAX_LENGTH, padding='post')\n","encoded_test_message = [one_hot(d, VOCAB_SIZE) for d in test_message]\n","padded_test_message = pad_sequences(encoded_test_message, maxlen=MAX_LENGTH)"],"metadata":{"id":"QGWam563Xjre"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing import sequence\n","\n","\n","tokens = Tokenizer(num_words=VOCAB_SIZE)\n","tokens.fit_on_texts(train_message)\n","\n","# Transform each text to a sequence of integers\n","train_sequence = tokens.texts_to_sequences(train_message)\n","train_sequences_matrix = sequence.pad_sequences(train_sequence, maxlen=MAX_LENGTH)\n","\n","\n","test_sequence = tokens.texts_to_sequences(test_message)\n","test_sequences_matrix = sequence.pad_sequences(test_sequence, maxlen=max_len)"],"metadata":{"id":"WKnzqaPyDXeO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(sequences_matrix[:5])"],"metadata":{"id":"F6juy2CzFKet"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Flatten, Embedding, Dense, Dropout, GlobalAveragePooling1D, LSTM, Input, Conv1D, GlobalMaxPooling1D\n","from keras.callbacks import EarlyStopping"],"metadata":{"id":"JomJ6EEe8LyJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["MODEL 1:"],"metadata":{"id":"05uk223f9-vg"}},{"cell_type":"code","source":["model_extra = Sequential()\n","model_extra.add(Input(shape=(MAX_LENGTH,)))\n","model_extra.add(Embedding(VOCAB_SIZE, 100))\n","model_extra.add(Flatten())\n","model_extra.add(Dense(16, activation='relu'))\n","model_extra.add(Dropout(0.5))\n","model_extra.add(Dense(1, activation='sigmoid'))\n","\n","model_extra.summary()"],"metadata":{"id":"cM8iAHJT8CPn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["MODEL 2:"],"metadata":{"id":"9931K_fE94sk"}},{"cell_type":"code","source":["model = tf.keras.Sequential()\n","model.add(Input(shape=(MAX_LENGTH,)))\n","model.add(Embedding(VOCAB_SIZE,50))\n","model.add(LSTM(64))\n","model.add(Dense(256, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(1,activation = 'sigmoid'))\n","\n","model.summary()"],"metadata":{"id":"lviaCetiQ6pE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(loss='binary_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n","monitor = EarlyStopping(monitor='val_loss', min_delta=1e-4, verbose=1, mode='min', restore_best_weights=True)\n","model.fit(train_sequences_matrix, train_label, validation_data=(test_sequences_matrix, test_label), callbacks=[monitor], epochs=10, verbose=2)"],"metadata":{"id":"A5UwtctTRGyw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# function to predict messages based on model\n","# (should return list containing prediction and label, ex. [0.008318834938108921, 'ham'])\n","def predict_message(pred_text):\n","  class_dict = {\n","      0 : \"ham\",\n","      1 : \"spam\",\n","      }\n","  X = pd.Series(pred_text)\n","  text = X.apply(lambda x: clean_txt(x))\n","  seq = t.texts_to_sequences(text)\n","  sequence_matrix = sequence.pad_sequences(seq, maxlen=max_len)\n","  prediction = [model.predict(sequence_matrix)[0][0], class_dict[np.round(model.predict(sequence_matrix)[0][0])]]\n","  return prediction\n","\n","pred_text = \"how are you doing today?\"\n","\n","prediction = predict_message(pred_text)\n","print(prediction)"],"metadata":{"id":"DfOLIsLo_AYE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dxotov85SjsC"},"outputs":[],"source":["# Run this cell to test your function and model. Do not modify contents.\n","def test_predictions():\n","  test_messages = [\"how are you doing today\",\n","                   \"sale today! to stop texts call 98912460324\",\n","                   \"i dont want to go. can we try it a different day? available sat\",\n","                   \"our new mobile video service is live. just install on your phone to start watching.\",\n","                   \"you have won Â£1000 cash! call to claim your prize.\",\n","                   \"i'll bring it tomorrow. don't forget the milk.\",\n","                   \"wow, is your arm alright. that happened to me one time too\"\n","                  ]\n","\n","  test_answers = [\"ham\", \"spam\", \"ham\", \"spam\", \"spam\", \"ham\", \"ham\"]\n","  passed = True\n","\n","  for msg, ans in zip(test_messages, test_answers):\n","    prediction = predict_message(msg)\n","    print(prediction)\n","    print(ans)\n","    if prediction[1] != ans:\n","      passed = False\n","\n","  if passed:\n","    print(\"You passed the challenge. Great job!\")\n","  else:\n","    print(\"You haven't passed yet. Keep trying.\")\n","\n","test_predictions()\n"]}],"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"https://github.com/freeCodeCamp/boilerplate-neural-network-sms-text-classifier/blob/master/fcc_sms_text_classification.ipynb","timestamp":1706010367207}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{}},"nbformat":4,"nbformat_minor":0}